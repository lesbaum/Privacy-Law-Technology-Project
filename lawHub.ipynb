{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import bert.tokenization as tokenization\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spicy\n",
    "import xlsxwriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=False)\n",
    "train=pd.read_csv(r\"C:\\Users\\peter\\Documents\\GitHub\\Privacy-Law-Technology-Project\\train.csv\")\n",
    "test=pd.read_csv(r\"C:\\Users\\peter\\Documents\\GitHub\\Privacy-Law-Technology-Project\\test.csv\")\n",
    "\n",
    "nlp = spicy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel(r'C:\\Users\\peter\\Documents\\GitHub\\Privacy-Law-Technology-Project\\filteredDataWithPrivacyPolicies.xlsx', index_col=0, na_values=['NA'])\n",
    "# print(excel_data.columns.ravel())\\n\n",
    "devs = excel_data[\"developer\"].tolist()\n",
    "policies = excel_data[\"Raw Privacy Policy\"].tolist()\n",
    "# print(nlp(policies[2]))\\n\",\n",
    "flag = 0\n",
    "chunkList = []\n",
    "combinedList = []\n",
    "for policy in policies:\n",
    "    doc = nlp(policy)\n",
    "    docList = list(doc.sents)\n",
    "    count = 0\n",
    "    #Takes in list of entire text and divides and stores chunks of 3 sentences. If we reach the end\\n\",\n",
    "    while (count < len(docList)):\n",
    "        if (count+2 < len(docList)):\n",
    "            chunkList.append([devs[flag], str(docList[count]) + \" \" +  str(docList[count+1]) + \" \" + str(docList[count+2])])\n",
    "            count = count + 3\n",
    "        elif(count+1 < len(docList)):\n",
    "            chunkList.append([devs[flag],str(docList[count]) + \" \" + str(docList[count+1])])\n",
    "            count = count + 2\n",
    "        elif(count < len(docList)):\n",
    "            chunkList.append([devs[flag],str(docList[count])])\n",
    "            count = count + 1\n",
    "    flag= flag + 1\n",
    "    # for elements in chunkList:\n",
    "    #     print(elements)\n",
    "    #     print(\"END OF CHUNK \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(chunkList)\n",
    "# writer = pd.ExcelWriter(r'C:\\Users\\peter\\Documents\\GitHub\\Privacy-Law-Technology-Projecttest.xlsx', engine='xlsxwriter')\n",
    "# df.to_excel(writer, sheet_name='welcome', index=False)\n",
    "# writer.close()\n",
    "chunkList=pd.read_csv(r\"C:\\Users\\peter\\Documents\\GitHub\\Privacy-Law-Technology-Project\\chunkedText.csv\")\n",
    "dropList = []\n",
    "for i in range(len(chunkList)):\n",
    "    if( len(chunkList.iloc[i][1]) < 40):\n",
    "        dropList.append(i)\n",
    "\n",
    "    chunkList.at[i,\"text\"] = re.sub('<[^<]+?>', '', chunkList.iloc[i][1])\n",
    "    chunkList.at[i,\"text\"] = re.sub('_[^_]+?_', '', chunkList.iloc[i][1])\n",
    "    chunkList.at[i, \"text\"] = re.sub('{[^{]+?}', '', chunkList.iloc[i][1])\n",
    "    \n",
    "chunkList.drop(dropList, axis=0, inplace=True) #clears out all html esque tags from text, as well as drops tags that state things such as \"couldn't access, couldn't open link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(2, activation='softmax')(net)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 150\n",
    "train_input = bert_encode(train.text.values, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(chunkList.text.values, tokenizer, max_len=max_len)\n",
    "train_labels =tf.keras.utils.to_categorical(train.label.astype('int32'), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     [(None, 1024),       335141889   ['input_word_ids[0][0]',         \n",
      "                                 (None, 150, 1024)]               'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1024)        0           ['keras_layer_1[0][1]']          \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           65600       ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            66          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335,209,635\n",
      "Trainable params: 67,746\n",
      "Non-trainable params: 335,141,889\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.5143 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.83333, saving model to model.h5\n",
      "3/3 [==============================] - 84s 22s/step - loss: 0.7101 - accuracy: 0.5143 - val_loss: 0.5942 - val_accuracy: 0.8333\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5793 - accuracy: 0.7571 \n",
      "Epoch 2: val_accuracy improved from 0.83333 to 0.86111, saving model to model.h5\n",
      "3/3 [==============================] - 66s 21s/step - loss: 0.5793 - accuracy: 0.7571 - val_loss: 0.4778 - val_accuracy: 0.8611\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.7357 \n",
      "Epoch 3: val_accuracy did not improve from 0.86111\n",
      "3/3 [==============================] - 64s 20s/step - loss: 0.5374 - accuracy: 0.7357 - val_loss: 0.4341 - val_accuracy: 0.8333\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4794 - accuracy: 0.7857 \n",
      "Epoch 4: val_accuracy did not improve from 0.86111\n",
      "3/3 [==============================] - 64s 20s/step - loss: 0.4794 - accuracy: 0.7857 - val_loss: 0.3722 - val_accuracy: 0.8611\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.7571 \n",
      "Epoch 5: val_accuracy improved from 0.86111 to 0.88889, saving model to model.h5\n",
      "3/3 [==============================] - 66s 21s/step - loss: 0.4585 - accuracy: 0.7571 - val_loss: 0.3373 - val_accuracy: 0.8889\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8214 \n",
      "Epoch 6: val_accuracy did not improve from 0.88889\n",
      "3/3 [==============================] - 67s 21s/step - loss: 0.3974 - accuracy: 0.8214 - val_loss: 0.3185 - val_accuracy: 0.8611\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8714 \n",
      "Epoch 7: val_accuracy did not improve from 0.88889\n",
      "3/3 [==============================] - 66s 21s/step - loss: 0.3771 - accuracy: 0.8714 - val_loss: 0.3271 - val_accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(bert_layer, max_len=max_len)\n",
    "model.summary()\n",
    "\n",
    "for layer in model.layers[-5:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels, \n",
    "    validation_split=0.2,\n",
    "    epochs=7,\n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 5082s 12s/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "chunkList['predicted_values'] = test_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k in chunkList.predicted_values:\n",
    "    if(abs(k[0] - k[1]) > .5):\n",
    "        if(k[0] > k[1]):\n",
    "            \n",
    "            chunkList.at[count,\"final_prediction\"] = 0\n",
    "        else:\n",
    "            chunkList.at[count,\"final_prediction\"] = 1\n",
    "    else:\n",
    "        chunkList.at[count,\"final_prediction\"] = 2\n",
    "    count = count + 1\n",
    "\n",
    "numRight = 0\n",
    "numTotal = len(chunkList)\n",
    "# for i in range(len(test)):\n",
    "#     if(test.iloc[i][\"label\"] == test.iloc[i][\"final_prediction\"]):\n",
    "#         numRight = numRight + 1\n",
    "#     elif(test.iloc[i][\"label\"] != 2 and test.iloc[i][\"final_prediction\"] ==2):\n",
    "#         numRight = numRight + 1\n",
    "\n",
    "# print(numRight/numTotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex.com\n",
    "nameRegex = re.compile(r\"[A-Z][a-z]+,?\\s+(?:[A-Z][a-z]*\\.?\\s*)?[A-Z][a-z]+\")\n",
    "\n",
    "#stackoverflow\n",
    "phoneNumberRegex = re.compile(r'\\+?\\d{1,4}?[-.\\s]?\\(?\\d{1,3}?\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}')\n",
    "#geeksforgeeks\n",
    "emailRegex = re.compile(r'\\S+@\\S+')\n",
    "#chatGPT\n",
    "addressRegex = re.compile(r'\\b\\d{1,5} [a-zA-Z0-9 \\-.,#&]*[a-zA-Z0-9]\\b')\n",
    "\n",
    "for j in range(len(chunkList)):\n",
    "    if (len(nameRegex.findall(str(chunkList.iloc[j][1]))) != 0):\n",
    "            chunkList.at[j,\"has_name\"] = 1\n",
    "    else:\n",
    "          chunkList.at[j,\"has_name\"] = 0\n",
    "    if (len(phoneNumberRegex.findall(str(chunkList.iloc[j][1]))) != 0):\n",
    "            chunkList.at[j,\"has_phone\"] = 1\n",
    "    else:\n",
    "          chunkList.at[j,\"has_phone\"] = 0\n",
    "    if (len(emailRegex.findall(str(chunkList.iloc[j][1]))) != 0):\n",
    "            chunkList.at[j,\"has_email\"] = 1\n",
    "    else:\n",
    "          chunkList.at[count,\"has_email\"] = 0\n",
    "    if (len(addressRegex.findall(str(chunkList.iloc[j][1]))) != 0):\n",
    "            chunkList.at[j,\"has_address\"] = 1\n",
    "    else:\n",
    "          chunkList.at[j,\"has_address\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avokiddo', 'Boomerang', 'Hitcents', 'Doodle Joy Studio', 'Intellijoy Educational Games for Kids', 'Sago Mini', 'Acram Digital sp z o.o.', 'Feral Interactive', 'Shimon Young : Play Home Software', 'Starfall Education Foundation', 'Dan Russell-Pinson', 'RosiMosi', 'Coloring Games', 'BabyBus', 'Homer Learning', 'Afterburn', 'Toca Boca', 'GunjanApps Studios', 'Lingokids - English Learning For Kids', 'My Town Games Ltd', 'Bubadu', 'Buggies Kids', 'Guinea Pig Games LLC', 'Nickelodeon', 'NimbleBit LLC', 'PBS KIDS', 'RV AppStudios', 'The Pinkfong Company', 'WildWorks', 'Classical Conversations Inc', 'Gameloft SE', 'IDZ Digital Private Limited', 'Libii HK Limited', 'Studio Pango - Kids Fun preschool learning games', 'Lazaros Dinakis', 'Sesame Workshop', 'Amanita Design', 'TutoTOONS', 'StoryToys', 'LEGO System A/S', 'Flamebait Games', 'Budge Studios', 'Hippo Kids Games', 'Entertainment One', 'codeSpark', 'XiHe Digital (GuangZhou) Technology Co., Ltd.', 'Star Stable Entertainment AB', 'Pazu Games', 'YovoGames', 'Bebi Family: preschool learning games for kids', 'Giant Fish', 'Torah Treasure', 'Ferrero Trading Lux S.A.', 'Pinna LLC', 'Blue Zoo', 'NHA GAMES', 'Fox & Sheep']\n",
      "Number of fully compliant companies: 57\n",
      "Number of Unique Companies:  232\n",
      "Number of Companies with Data Collector Names:  203\n",
      "Number of companies with Data Collector Email: 142\n",
      "Number of Companies with Data Collector Phone Number:  156\n",
      "Number of Companies with Data Collector Address:  163\n",
      "Number of Companies who are compliant with section 2:  140\n",
      "Number of Companies who are compliant with section 3:  99\n"
     ]
    }
   ],
   "source": [
    "section3compliance = chunkList[chunkList['final_prediction'] == 1].groupby('company').size()\n",
    "numSect3Compliant = len(section3compliance)\n",
    "sect3companies = section3compliance.index.tolist()\n",
    "\n",
    "section2compliance = chunkList[chunkList['final_prediction'] == 0].groupby('company').size()\n",
    "numSect2compliance = len(section2compliance)\n",
    "sect2companies = section2compliance.index.tolist()\n",
    "\n",
    "sect1namecompliance = chunkList[chunkList['has_name'] == 1].groupby('company').size()\n",
    "numNamecompliance = len(sect1namecompliance)\n",
    "sect1namecompanies = sect1namecompliance.index.tolist()\n",
    "\n",
    "sect1emailcompliance = chunkList[chunkList['has_email'] == 1].groupby('company').size()\n",
    "numEmailcompliance = len(sect1emailcompliance)\n",
    "sect1emailcompanies = sect1emailcompliance.index.tolist()\n",
    "\n",
    "sect1phonecompliance = chunkList[chunkList['has_phone'] == 1].groupby('company').size()\n",
    "numPhonecompliance = len(sect1phonecompliance)\n",
    "sect1phonecompanies = sect1phonecompliance.index.tolist()\n",
    "\n",
    "sect1addresscompliance = chunkList[chunkList['has_address'] == 1].groupby('company').size()\n",
    "numAddresscompliance = len(sect1addresscompliance)\n",
    "sect1addresscompanies = sect1addresscompliance.index.tolist()\n",
    "\n",
    "\n",
    "common_elements = list(\n",
    "    set(sect3companies).intersection(sect2companies, sect1namecompanies,sect1emailcompanies, sect1phonecompanies, sect1addresscompanies)\n",
    ")\n",
    "print(common_elements)\n",
    "print(\"Number of fully compliant companies:\", len(common_elements))\n",
    "\n",
    "print( \"Number of Unique Companies: \", chunkList['company'].nunique())\n",
    "print( \"Number of Companies with Data Collector Names: \", numNamecompliance)\n",
    "print( \"Number of companies with Data Collector Email:\", numEmailcompliance )\n",
    "print(\"Number of Companies with Data Collector Phone Number: \", numPhonecompliance)\n",
    "print(\"Number of Companies with Data Collector Address: \", numAddresscompliance)\n",
    "print(\"Number of Companies who are compliant with section 2: \", numSect2compliance)\n",
    "print(\"Number of Companies who are compliant with section 3: \", numSect3Compliant)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sarah Jessica Sebastian', 'Sebastian Rivera', 'Ben Lepsch']\n",
      "['+1 (630)-200-9399']\n",
      "['kyle@7thbe.at', 'href=mailto:app_support@rvappstudios.com', '>app_support@rvappstudios.com</a>_x000D_']\n",
      "['7190 Billund, Denmark  18 rue', '92120 Montrouge', '707 North Dubuque St']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#regex.com\n",
    "string = \"A cookie is a Sarah Jessica Sebastian Parker string of text information Sebastian Rivera transferred from a website to your computer's hard drive so that Ben Lepsch the website can remember you. Cookies can help a website adapt content more quickly to your interests. Most importantly websites use cookies. In general, a cookie contains the name of the domain of origin of the cookie; the of the cookie and a value, namely a unique number created randomly.\"\n",
    "regex = re.compile(r\"[A-Z][a-z]+,?\\s+(?:[A-Z][a-z]*\\.?\\s*)?[A-Z][a-z]+\")\n",
    "print(regex.findall(string))\n",
    "#stackoverflow\n",
    "phoneNumberRegex = re.compile(r'\\+?\\d{1,4}?[-.\\s]?\\(?\\d{1,3}?\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}')\n",
    "string2 = \"call me at Telephone +1 (630)-200-9399\"\n",
    "print(phoneNumberRegex.findall(string2))\n",
    "#geeksforgeeks\n",
    "emailRegex = re.compile(r'\\S+@\\S+')\n",
    "string3 = \"If If you have any questions or suggestions about our Privacy Policy, do not hesitate to contact us at kyle@7thbe.at contact us at the following email address.</span><br/> <a href=mailto:app_support@rvappstudios.com style= color:#38B9ECgame-polic >app_support@rvappstudios.com</a>_x000D_\"\n",
    "print(emailRegex.findall(string3))\n",
    "#chatGPT\n",
    "addressRegex = re.compile(r'\\b\\d{1,5} [a-zA-Z0-9 \\-.,#&]*[a-zA-Z0-9]\\b')\n",
    "string4 = \"my address is Aastvej 1, 7190 Billund, Denmark  18 rue Barbès 92120 Montrouge (France) 707 North Dubuque St\"\n",
    "print(addressRegex.findall(string4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3:16 Apps', '4Axis Technologies', 'A4tune Labs', 'Acram Digital sp z o.o.', 'Afterburn', 'Age of Learning, Inc.', 'Aha World Ltd.', 'Amanita Design', 'Amaya Kids - learning games for 3-5 years old', 'Amaziograph Ltd.', 'Amazon Mobile LLC', 'Ambient Design Ltd.', 'Angel Studios, Inc.', 'Anime Dress Up Games', 'Anishu, Inc.', 'App Family Kids - Games for boys and girls', 'AppQuiz', 'Apps by Mr. B.', 'Avaz Inc.', 'Avokiddo', 'BabyBus', 'Baram FZE', 'Bebi Family: preschool learning games for kids', 'Bimi Boo Kids Learning Games for Toddlers FZ-LLC', 'Bini Games', 'Blue Zoo', 'Boomerang', 'Bubadu', 'Budge Studios', 'Buggies Kids', 'COOL NIKS', 'Cahokia Mounds Museum Society', 'Candy Rufus Games', 'Cartoon Network', 'Chess.com', 'ClassDojo', 'Classical Conversations Inc', 'Coloring Games', \"Crab's Games\", 'Crayola LLC', 'Dan Russell-Pinson', 'Dekoa Apps', 'Dinosaur Polo Club', 'Disney', 'Doodle Joy Studio', 'Dr. Panda', 'Dress Up Makeover Girls Games', 'Dry Cactus Limited', 'Duolingo', 'EDOKI ACADEMY', 'Egg Splat Studios', 'Entertainment One', 'Epic Story Interactive', 'Everett Kaser', 'Feral Interactive', 'Ferrero Trading Lux S.A.', 'Flamebait Games', 'Fox & Sheep', 'FunEasyLearn', 'GIANTS Software', 'Gameloft SE', 'Ganz TM', 'Giant Fish', 'GoBit Games', 'GoNoodle', 'Guinea Pig Games LLC', 'GunjanApps Studios', 'Hasbro Inc.', 'Hey Clay', 'Hippo Kids Games', 'HitGrab Game Studio', 'Hitcents', 'Homer Learning', 'Hooked on Phonics', 'Humongous Entertainment', 'IDZ Digital Private Limited', 'Infinity Games, Lda', 'IntellectoKids Ltd', 'Intellijoy Educational Games for Kids', 'Joel McDonald', 'Journey Bound Games Inc.', 'JusTalk', 'Kahoot!', 'Kidly - Stories for Kids', 'LEGO System A/S', 'Language Skills Studio', 'Lazaros Dinakis', 'Letterschool Enabling Learning', 'Libii HK Limited', 'Life.Church', 'Lingokids - English Learning For Kids', 'LogicLike - Educational Games', 'Lowtech Studios', 'Makkajai: Math Games for 1st, 2nd, 3rd, 4th grade', 'MentalUP - Learning Games for Kids', 'Meta Platforms, Inc.', 'Mind Candy Ltd', 'Mindware Consulting, Inc', 'Minibuu', 'Minno Kids', 'My Town Games Ltd', 'NHA GAMES', 'Nickelodeon', 'NimbleBit LLC', 'Originator Inc.', 'PBS KIDS', 'Pavel Zinchenko', 'Pazu Games', 'Pepi Play', 'Photomath, Inc.', 'Pinna LLC', 'PlayDate Digital Inc.', 'PlayMonster LLC', 'PlayWay SA', 'Playables', 'Prodigy Education Inc.', 'RED BUTTON LLC', 'RV AppStudios', 'Raw Fury', 'Readability Reading Tutor', 'Romans I XVI Gaming', 'RosiMosi', 'SEGA', 'Sago Mini', 'Sesame Workshop', 'Shimon Young : Play Home Software', 'Skyship Entertainment Company', 'Slime Games for Girls', 'So Far So Good', 'Software Studios', 'Star Stable Entertainment AB', 'Starfall Education Foundation', 'Steely Systems', 'Stone Golem Studios', 'StoryToys', 'Studio Pango - Kids Fun preschool learning games', 'THUP Games', 'Teach Your Monster', 'Teenage Fashion Dress Up', 'The Christian Broadcasting Network (CBN)', 'The Pinkfong Company', 'The Pokémon Company', 'TinyTap', 'Toca Boca', 'Tomorrow Corporation', 'TooBZ LLC', 'Torah Treasure', 'True Axis', 'TutoTOONS', 'Viva Games Studios', 'Vooks Inc', 'Warner Bros. International Enterprises', 'WildWorks', 'Wolfoo LLC', 'Wow Kids', 'WriteOn', 'XiHe Digital (GuangZhou) Technology Co., Ltd.', 'YovoGames', 'codeSpark', 'gamesfarm', 'grapefrukt games', 'shredderchess.com', 'trochoi ltd co']\n"
     ]
    }
   ],
   "source": [
    "print(sect1addresscompliance.index.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
